{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci splotowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "Ponownie wykorzystamy w zadaniu zbiór MNIST. Zaczynamy od pobrania dnaych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "target_directory = \"mnist\"\n",
    "\n",
    "mnist_real_train = MNIST(target_directory, train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = MNIST(target_directory, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_validation = data.random_split(mnist_real_train, (48000, 12000))\n",
    "len(mnist_train), len(mnist_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, expected):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == expected).type(torch.float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa sieci neuronowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa splotowa\n",
    "\n",
    "Warstwa splotowa (ang. *convolutional layer*) przesuwa jądro (ang. *kernel*) po obrazie (w ogólności: po macierzy) miejsce przy miejscu, oblicza wynik i zapamiętuje w macierzy wyjściowej. Na poniższym rysunku mamy do czynienia z obrazem $7\\times 5$ pikseli (jasnoszary obszar w dolnej części rysunku) po którym przesuwane jest jądro $3\\times 3$ piksele. Obrazek jest uzupełniony (ang. *padding*) o zera, zaznaczone na rysunku kolorem ciemnoszarym. Po co takie uzupełnienie? Bez niego macierz wynikowa byłaby mniejsza niż obrazek wejściowy. Czerwony piksel w macierzy wyjściowej obliczany jest następująco: każdy z pikseli obrazu wejściowego jest mnożony przez odpowiadającą mu wartość w filtrze, a tak uzyskane 9 wartości jest sumowane. Następnie ten sam filtr wykorzystywany jest do obliczenia kolejnego piksela - zaznaczonego na niebiesko - na podstawie pikseli wejściowych zaznaczonych niebieską linią przerywaną. Filtr to są wagi splotowej sieci neuronowej, to znaczy to on podlega uczeniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rysunek poglądowy dotyczący warstwy splotowej](img/mlst_1303.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jądro nie musi przsuwać się za każdym razem o 1 piksel, może mieć większy krok (ang. *stride*), jak na poniższym rysunku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ilustracja *stride*](img/mlst_1304.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ale, ale... przecież obrazki mogą być kolorowe! No i faktycznie, możemy pojedynczy postrzegać pojedynczy obrazek jako kilka obrazków monochromatycznych. Wtedy zamiast przesuwać płaski filtr po pojedynczym obrazku przesuwamy kostkę, której dwa wymiary możemy modyfikować (odpowiedzialne za wysokość i szerokość), natomiast jej głębokość jest ustalona - zależy od liczby warstw obrazka. Na poniższym rysunku zaczynamy od normalnego obrazka RGB, który następnie jest zamieniany na wiele map - każda posiadająca swój jeden filtr (kostkę) - w ten sposób powstaje nowy \"obrazek\" podpisany na rysunku *Convolutional layer 1*, na którym... ponawiamy operację! Używając nowego zestawu filtrów tworzymy *Convolutional layer 2*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ilustracja sieci splotowej w przypadku trójwymiarowym](img/mlst_1306.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwę splotową w PyTorch realizuje klasa [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d). Pierwsze trzy parametry jej konstruktora są obowiązkowe, to kolejno: liczba map na wejściu, liczba map na wyjściu, rozmiar jądra (jedna liczba jeżeli ma być kwadratowe albo para liczb jeżeli ma być prostokątem).\n",
    "\n",
    "Będziemy budowali krok po kroku tablicę `layers`, której będziemy umieszczali kolejne warstwy naszej sieci neuronowej.\n",
    "Rozpoczniemy od dodania warstwy splotowej.\n",
    "Obrazki MNIST są monochromatyczne, więc mamy tylko 1 kanał wejściowy.\n",
    "Przyjmimy, że na wyjściu będziemy mieli 5 map, każdą na bazie kwadratowego filtra o boku 3.\n",
    "Żeby nie zmniejszyć zbyt szybko obrazka dodamy po 1 pikselu paddingu z każdej strony - jak na rysunku powyżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    nn.Conv2d(1, 5, 3, padding=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nieliniowość\n",
    "\n",
    "Warstwa splotowa - podobnie jak warstwa liniowa (`nn.Linear`) - jest tylko sumą. Nie ma sensu bezpośrednie składanie kilku warstw splotowych jedna na drugą, potrzebna jest nieliniowość. Zastosujemy *leaky ReLU*, funkcję aktywacji przedstawioną na poniższym wykresie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Leaky ReLU](img/leaky_relu.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.LeakyReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Łączenie\n",
    "\n",
    "Sieć splotowa pozwala uwydatnić pewne cechy w obrazie, na przykład na poniższym rysunku na mapie po lewej stronie uwydatnione zostały linie pionowe, a po prawej linie poziome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/mlst_1305.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z drugiej strony to uwydatnienie spowodowało, że trochę straciliśmy szczegóły i właściwie nie potrzebujemy już tak dużej rozdzielczości. Z pomocą przychodzi operacja łączenia (ang. *pooling*), która podobnie jak sieć splotowa przesuwa filtr przez obraz, ale ten filtr jest pozbawiony parametrów: służy albo do wybierania maksimum (ang. *max pooling*) albo do obliczania średniej arytmetycznej (ang. *average pooling*). Poniższy rysunek zgrubnie prezentuje ideę *max pooling* (kernel $2\\times 2$, stride 2, bez paddingu - stąd $\\times$ w ostatniej kolumnie). W przypadku łączenia nie ma problemu z przejściem do obrazów kolorowych - każda mapa analizowana jest oddzielnie, więc zawsze pozostajemy w dwóch wymiarach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max pooling](img/mlst_1308.png)\n",
    "\n",
    "Aurélien Géron \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" O'Reilly Media 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodamy do naszej sieci neuronowej *max pooling*, realizowany za pomocą klasy `nn.MaxPool2d`, z jądrem rozmiaru $3\\times 3$ i uzupełnieniem o 1 piksel z każdej strony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.MaxPool2d(3, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "\n",
    "Ile map i jakiego rozmiaru będzie na tym etapie przetwarzania, jeżeli wejście miało jedną mapę rozmiaru $28\\times 28$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Liczba map**: ... \n",
    "* **Rozmiar każdej mapy**: ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spłaszczanie\n",
    "\n",
    "Na tym etapie każdy obiekt przetwarzany przez naszą sieć neuronową jest trójwymiarowy, tj. składa się z pewnej liczby dwuwymiarowych map. Takiego wejścia nie możemy podać do warstwy liniowej! Wykorzystamy klasę `nn.Flatten`, żeby ułożyć piksele jeden za drugim w formie wektora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2: Warstwa liniowa\n",
    "\n",
    "Tak samo jak na poprzednich zajęciach wykorzystamy do klasyfikacji warstwę `nn.Linear`. Klasyfikujemy do 10 klas, więc musimy mieć 10 neuronów, ale ilu wejść potrzebujemy? Uzupełnij poprzedni kod na podstawie wyników zadania 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.Linear(..., 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faktyczna budowa sieci neuronowej\n",
    "\n",
    "Warstwy połączymy w jeden moduł wykorzystując klasę `nn.Sequential`, jako funkcję straty wykorzystamy entropię krzyżową, a do optymalizacji optymalizator Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*layers)\n",
    "\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie z wykorzystaniem wczesnego zatrzymania\n",
    "\n",
    "Wczesne zatrzymanie (ang. *early stopping*) polega na tym, że co określoną liczbę epok uczenia obliczamy miarę oceny (np. trafność klasyfikacji) na zbiorze walidującym. Jeżeli nastąpiła poprawa w stosunku do poprzedniego razu, to zapamiętujemy obecne wartości wag sieci neuronowej, jeżeli zaś przez kilka razy nie następuje poprawa, to przerywamy uczenie i przywracamy wagi ostatniego najlepszego modelu.\n",
    "\n",
    "Poniższy kawałek kodu oblicza trafność na zbiorze walidującym co epokę i przerywa uczenie jeżeli przez 5 kolejnych epok nie nastąpiła poprawa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "validation_acc = []\n",
    "best_model = None\n",
    "best_acc = None\n",
    "best_epoch = None\n",
    "max_epoch = 10000\n",
    "no_improvement = 5\n",
    "batch_size = 512\n",
    "\n",
    "for n_epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    loader = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    epoch_loss = []\n",
    "    for X_batch, y_batch in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = cost(logits, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        epoch_loss.append(loss.detach())\n",
    "    train_loss.append(torch.tensor(epoch_loss).mean())\n",
    "    model.eval()\n",
    "    loader = data.DataLoader(mnist_validation, batch_size=len(mnist_validation), shuffle=False)\n",
    "    X, y = next(iter(loader))\n",
    "    logits = model(X)\n",
    "    acc = compute_acc(logits, y).detach()\n",
    "    validation_acc.append(acc)\n",
    "    if best_acc is None or acc > best_acc:\n",
    "        print(\"New best epoch \", n_epoch, \"acc\", acc)\n",
    "        best_acc = acc\n",
    "        best_model = model.state_dict()\n",
    "        best_epoch = n_epoch\n",
    "    if best_epoch + no_improvement <= n_epoch:\n",
    "        print(\"No improvement for\", no_improvement, \"epochs\")\n",
    "        break\n",
    "        \n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyj się poniższym wykresom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation accuracy. Dot denotes best accuracy.')\n",
    "plt.plot(validation_acc, label='Validation accuracy')\n",
    "plt.plot(best_epoch, best_acc, 'bo', label='Best accuracy')\n",
    "plt.show()\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_loss)\n",
    "plt.show()\n",
    "k = max(3*no_improvement, 0)\n",
    "plt.title('Last {} epochs'.format(k))\n",
    "plt.plot(validation_acc[-k:])\n",
    "plt.plot(best_epoch-(len(validation_acc)-k), best_acc, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "Dlaczego wczesne zatrzymanie jest realizowane na zbiorze walidującym, a nie na zbiorze uczącym albo na zbiorze testowym?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tu miejsce na odpowiedź...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4\n",
    "\n",
    "Oblicz trafność klasyfikacji na zbiorze walidującym i na zbiorze testowym. Czy uzyskane wartości się różnią? W którą stronę? Dlaczego tak może być?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu napisz odpowiedni kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A tu napisz komentarz**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie do samodzielnego wykonania\n",
    "\n",
    "Zaimplementuj sieć o architekturze zbliżonej do [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), zgodnie z poniższym opisem:\n",
    "\n",
    "|Nr warstwy|Typ|Rozmiar wyjścia|Liczba filtrów|Rozmiar jądra (`kernel_size`)|Krok (`stride`)|Padding|Funkcja aktywacji|\n",
    "|----------|---|---------------|--------------|-----------------------|---------------|-------|-----------------|\n",
    "|1|splotowa|$28\\times 28$|6|$5\\times 5$|1|2|tanh|\n",
    "|2|avgerage pooling|$14\\times 14$|6|$2\\times 2$|2|0|tanh|\n",
    "|3|splotowa|$10\\times 10$|16|$5\\times 5$|1|0|tanh|\n",
    "|4|average pooling|$5\\times 5$|16|$2\\times 2$|2|0|tanh|\n",
    "|5|splotowa|$1\\times 1$|120|$5\\times 5$|1|0|tanh|\n",
    "|6|pełna|84|||||tanh|\n",
    "|7|pełna|10|||||brak|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architektura w formie rysunku (z [oryginalnego artykułu](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)):\n",
    "\n",
    "![LeNet5 architecture](img/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania szczegółowe:\n",
    "\n",
    "1. Zbuduj sieć\n",
    "2. Ucz sieć wykorzystując early stopping (przez ocenę accuracy na zbiorze walidującym)\n",
    "3. Oceń jakość sieci na zbiorze testowym\n",
    "4. Wyświetl 10 źle zaklasyfikowanych przypadków ze zbioru testowego, razem z informacją o poprawnej etykiecie oraz etykiecie zaproponowanej przez model. Posłuż się funkcją `plt.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
